<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Noah Greifer" />

<meta name="date" content="2025-05-29" />

<title>Matching Methods</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Matching Methods</h1>
<h4 class="author">Noah Greifer</h4>
<h4 class="date">2025-05-29</h4>


<div id="TOC">
<ul>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#matching" id="toc-matching">Matching</a></li>
<li><a href="#matching-methods" id="toc-matching-methods">Matching
Methods</a>
<ul>
<li><a href="#nearest-neighbor-matching-method-nearest" id="toc-nearest-neighbor-matching-method-nearest">Nearest Neighbor
Matching (<code>method = &quot;nearest&quot;</code>)</a></li>
<li><a href="#optimal-pair-matching-method-optimal" id="toc-optimal-pair-matching-method-optimal">Optimal Pair Matching
(<code>method = &quot;optimal&quot;</code>)</a></li>
<li><a href="#optimal-full-matching-method-full" id="toc-optimal-full-matching-method-full">Optimal Full Matching
(<code>method = &quot;full&quot;</code>)</a></li>
<li><a href="#generalized-full-matching-method-quick" id="toc-generalized-full-matching-method-quick">Generalized Full
Matching (<code>method = &quot;quick&quot;</code>)</a></li>
<li><a href="#genetic-matching-method-genetic" id="toc-genetic-matching-method-genetic">Genetic Matching
(<code>method = &quot;genetic&quot;</code>)</a></li>
<li><a href="#exact-matching-method-exact" id="toc-exact-matching-method-exact">Exact Matching
(<code>method = &quot;exact&quot;</code>)</a></li>
<li><a href="#coarsened-exact-matching-method-cem" id="toc-coarsened-exact-matching-method-cem">Coarsened Exact Matching
(<code>method = &quot;cem&quot;</code>)</a></li>
<li><a href="#subclassification-method-subclass" id="toc-subclassification-method-subclass">Subclassification
(<code>method = &quot;subclass&quot;</code>)</a></li>
<li><a href="#cardinality-and-profile-matching-method-cardinality" id="toc-cardinality-and-profile-matching-method-cardinality">Cardinality
and Profile Matching (<code>method = &quot;cardinality&quot;</code>)</a></li>
</ul></li>
<li><a href="#customizing-the-matching-specification" id="toc-customizing-the-matching-specification">Customizing the Matching
Specification</a>
<ul>
<li><a href="#specifying-the-propensity-score-or-other-distance-measure-distance" id="toc-specifying-the-propensity-score-or-other-distance-measure-distance">Specifying
the propensity score or other distance measure
(<code>distance</code>)</a></li>
<li><a href="#implementing-common-support-restrictions-discard" id="toc-implementing-common-support-restrictions-discard">Implementing
common support restrictions (<code>discard</code>)</a></li>
<li><a href="#caliper-matching-caliper" id="toc-caliper-matching-caliper">Caliper matching
(<code>caliper</code>)</a></li>
<li><a href="#mahalanobis-distance-matching-mahvars" id="toc-mahalanobis-distance-matching-mahvars">Mahalanobis distance
matching (<code>mahvars</code>)</a></li>
<li><a href="#exact-matching-exact" id="toc-exact-matching-exact">Exact
matching (<code>exact</code>)</a></li>
<li><a href="#anti-exact-matching-antiexact" id="toc-anti-exact-matching-antiexact">Anti-exact matching
(<code>antiexact</code>)</a></li>
<li><a href="#matching-with-replacement-replace" id="toc-matching-with-replacement-replace">Matching with replacement
(<code>replace</code>)</a></li>
<li><a href="#k1-matching-ratio" id="toc-k1-matching-ratio"><span class="math inline">\(k\)</span>:1 matching
(<code>ratio</code>)</a></li>
<li><a href="#matching-order-m.order" id="toc-matching-order-m.order">Matching order
(<code>m.order</code>)</a></li>
</ul></li>
<li><a href="#choosing-a-matching-method" id="toc-choosing-a-matching-method">Choosing a Matching Method</a></li>
<li><a href="#reporting-the-matching-specification" id="toc-reporting-the-matching-specification">Reporting the Matching
Specification</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><code>MatchIt</code> implements several matching methods with a
variety of options. Though the help pages for the individual methods
describe each method and how they can be used, this vignette provides a
broad overview of the available matching methods and their associated
options. The choice of matching method depends on the goals of the
analysis (e.g., the estimand, whether low bias or high precision is
important) and the unique qualities of each dataset to be analyzed, so
there is no single optimal choice for any given analysis. A benefit of
nonparametric preprocessing through matching is that a number of
matching methods can be tried and their quality assessed without
consulting the outcome, reducing the possibility of capitalizing on
chance while allowing for the benefits of an exploratory analysis in the
design phase <span class="citation">(<a href="#ref-ho2007">Ho et al.
2007</a>)</span>.</p>
<p>This vignette describes each matching method available in
<code>MatchIt</code> and the various options that are allowed with
matching methods and the consequences of their use. For a brief
introduction to the use of <code>MatchIt</code> functions, see
<code>vignette(&quot;MatchIt&quot;)</code>. For details on how to assess and
report covariate balance, see
<code>vignette(&quot;assessing-balance&quot;)</code>. For details on how to
estimate treatment effects and standard errors after matching, see
<code>vignette(&quot;estimating-effects&quot;)</code>.</p>
</div>
<div id="matching" class="section level2">
<h2>Matching</h2>
<p>Matching as implemented in <code>MatchIt</code> is a form of
<em>subset selection</em>, that is, the pruning and weighting of units
to arrive at a (weighted) subset of the units from the original dataset.
Ideally, and if done successfully, subset selection produces a new
sample where the treatment is unassociated with the covariates so that a
comparison of the outcomes treatment and control groups is not
confounded by the measured and balanced covariates. Although statistical
estimation methods like regression can also be used to remove
confounding due to measured covariates, <span class="citation">Ho et al.
(<a href="#ref-ho2007">2007</a>)</span> argue that fitting regression
models in matched samples reduces the dependence of the validity of the
estimated treatment effect on the correct specification of the
model.</p>
<p>Matching is nonparametric in the sense that the estimated weights and
pruning of the sample are not direct functions of estimated model
parameters but rather depend on the organization of discrete units in
the sample; this is in contrast to propensity score weighting (also
known as inverse probability weighting), where the weights come more
directly from the estimated propensity score model and therefore are
more sensitive to its correct specification. These advantages, as well
as the intuitive understanding of matching by the public compared to
regression or weighting, make it a robust and effective way to estimate
treatment effects.</p>
<p>It is important to note that this implementation of matching differs
from the methods described by Abadie and Imbens <span class="citation">(<a href="#ref-abadie2006">2006</a>, <a href="#ref-abadie2016">2016</a>)</span> and implemented in the
<code>Matching</code> R package and <code>teffects</code> routine in
Stata. That form of matching is <em>matching imputation</em>, where the
missing potential outcomes for each unit are imputed using the observed
outcomes of paired units. This is a critical distinction because
matching imputation is a specific estimation method with its own effect
and standard error estimators, in contrast to subset selection, which is
a preprocessing method that does not require specific estimators and is
broadly compatible with other parametric and nonparametric analyses. The
benefits of matching imputation are that its theoretical properties
(i.e., the rate of convergence and asymptotic variance of the estimator)
are well understood, it can be used in a straightforward way to estimate
not just the average treatment effect in the treated (ATT) but also the
average treatment effect in the population (ATE), and additional
effective matching methods can be used in the imputation (e.g., kernel
matching). The benefits of matching as nonparametric preprocessing are
that it is far more flexible with respect to the types of effects that
can be estimated because it does not involve any specific estimator, its
empirical and finite-sample performance has been examined in depth and
is generally well understood, and it aligns well with the design of
experiments, which are more familiar to non-technical audiences.</p>
<p>In addition to subset selection, matching often (though not always)
involves a form of <em>stratification</em>, the assignment of units to
pairs or strata containing multiple units. The distinction between
subset selection and stratification is described by <span class="citation">Zubizarreta, Paredes, and Rosenbaum (<a href="#ref-zubizarretaMatchingBalancePairing2014">2014</a>)</span>, who
separate them into two separate steps. In <code>MatchIt</code>, with
almost all matching methods, subset selection is performed by
stratification; for example, treated units are paired with control
units, and unpaired units are then dropped from the matched sample. With
some methods, subclasses are used to assign matching or stratification
weights to individual units, which increase or decrease each unit’s
leverage in a subsequent analysis. There has been some debate about the
importance of stratification after subset selection; while some authors
have argued that, with some forms of matching, pair membership is
incidental <span class="citation">(<a href="#ref-stuart2008">Stuart
2008</a>; <a href="#ref-schafer2008">Schafer and Kang 2008</a>)</span>,
others have argued that correctly incorporating pair membership into
effect estimation can improve the quality of inferences <span class="citation">(<a href="#ref-austin2014a">Austin and Small 2014</a>;
<a href="#ref-wan2019">Wan 2019</a>)</span>. For methods that allow it,
<code>MatchIt</code> includes stratum membership as an additional output
of each matching specification. How these strata can be used is detailed
in <code>vignette(&quot;estimating-effects&quot;)</code>.</p>
<p>At the heart of <code>MatchIt</code> are three classes of methods:
distance matching, stratum matching, and pure subset selection.
<em>Distance matching</em> involves considering a focal group (usually
the treated group) and selecting members of the non-focal group (i.e.,
the control group) to pair with each member of the focal group based on
the <em>distance</em> between units, which can be computed in one of
several ways. Members of either group that are not paired are dropped
from the sample. Nearest neighbor matching
(<code>method = &quot;nearest&quot;</code>), optimal pair matching
(<code>method = &quot;optimal&quot;</code>), optimal full matching
(<code>method = &quot;full&quot;</code>), generalized full matching
(<code>method = &quot;quick&quot;</code>), and genetic matching
(<code>method = &quot;genetic&quot;</code>) are the methods of distance matching
implemented in <code>MatchIt</code>. Typically, only the average
treatment in the treated (ATT) or average treatment in the control
(ATC), if the control group is the focal group, can be estimated after
distance matching in <code>MatchIt</code> (full matching is an
exception, described later).</p>
<p><em>Stratum matching</em> involves creating strata based on unique
values of the covariates and assigning units with those covariate values
into those strata. Any units that are in strata that lack either treated
or control units are then dropped from the sample. Strata can be formed
using the raw covariates (<code>method = &quot;exact&quot;</code>), coarsened
versions of the covariates (<code>method = &quot;cem&quot;</code>), or coarsened
versions of the propensity score (<code>method = &quot;subclass&quot;</code>).
When no units are discarded, either the ATT, ATC, or ATE can be
estimated after stratum matching, though often some units are discarded,
especially with exact and coarsened exact matching, making the estimand
less clear. For use in estimating marginal treatment effects after exact
matching, stratification weights are computed for the matched units
first by computing a new “stratum propensity score” for each unit, which
is the proportion of treated units in its stratum. The formulas for
computing inverse probability weights from standard propensity scores
are then applied to the new stratum propensity scores to form the new
weights.</p>
<p>Pure subset selection involves selecting a subset of units form the
original sample without considering the distance between individual
units or strata that units might fall into. Subsets are selected to
optimize a criterion subject to constraint on balance and remaining
sample size. Cardinality and profile matching
(<code>method = &quot;cardinality&quot;</code>) are the methods of pure subset
selection implemented in <code>MatchIt</code>. Both methods allow the
user to specify the largest imbalance allowed in the resulting matched
sample, and an optimization routine attempts to find the largest matched
sample that satisfies those balance constraints. While cardinality
matching does not target a specific estimand, profile matching can be
used to target the ATT, ATC, or ATE.</p>
<p>Below, we describe each of the matching methods implemented in
<code>MatchIt</code>.</p>
</div>
<div id="matching-methods" class="section level2">
<h2>Matching Methods</h2>
<div id="nearest-neighbor-matching-method-nearest" class="section level3">
<h3>Nearest Neighbor Matching (<code>method = &quot;nearest&quot;</code>)</h3>
<p>Nearest neighbor matching is also known as greedy matching. It
involves running through the list of treated units and selecting the
closest eligible control unit to be paired with each treated unit. It is
greedy in the sense that each pairing occurs without reference to how
other units will be or have been paired, and therefore does not aim to
optimize any criterion. Nearest neighbor matching is the most common
form of matching used <span class="citation">(<a href="#ref-thoemmes2011">Thoemmes and Kim 2011</a>; <a href="#ref-zakrison2018">Zakrison, Austin, and McCredie 2018</a>)</span>
and has been extensively studied through simulations. See
<code>?method_nearest</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;nearest&quot;</code>.</p>
<p>Nearest neighbor matching requires the specification of a distance
measure to define which control unit is closest to each treated unit.
The default and most common distance is the <em>propensity score
difference</em>, which is the difference between the propensity scores
of each treated and control unit <span class="citation">(<a href="#ref-stuart2010">Stuart 2010</a>)</span>. Another popular distance
is the Mahalanobis distance, described in the section “Mahalanobis
distance matching” below. The order in which the treated units are to be
paired must also be specified and has the potential to change the
quality of the matches <span class="citation">(<a href="#ref-austin2013b">Austin 2013</a>; <a href="#ref-rubin1973">Rubin
1973</a>)</span>; this is specified by the <code>m.order</code>
argument. With propensity score matching, the default is to go in
descending order from the highest propensity score; doing so allows the
units that would have the hardest time finding close matches to be
matched first <span class="citation">(<a href="#ref-rubin1973">Rubin
1973</a>)</span>. Other orderings are possible, including random
ordering, which can be tried multiple times until an adequate matched
sample is found. When matching with replacement (i.e., where each
control unit can be reused to be matched with any number of treated
units), the matching order doesn’t matter.</p>
<p>When using a matching ratio greater than 1 (i.e., when more than 1
control units are requested to be matched to each treated unit),
matching occurs in a cycle, where each treated unit is first paired with
one control unit, and then each treated unit is paired with a second
control unit, etc. Ties are broken deterministically based on the order
of the units in the dataset to ensure that multiple runs of the same
specification yield the same result (unless the matching order is
requested to be random).</p>
<p>Nearest neighbor matching is implemented in <code>MatchIt</code>
using internal C++ code through <code>Rcpp</code>. When matching on a
propensity score, this makes matching extremely fast, even for large
datasets. Using a caliper on the propensity score (described below)
makes it even faster. Run times may be a bit longer when matching on
other distance measures (e.g., the Mahalanobis distance). In contrast to
optimal pair matching (described below), nearest neighbor matching does
not require computing the full distance matrix between units, which
makes it more applicable to large datasets.</p>
</div>
<div id="optimal-pair-matching-method-optimal" class="section level3">
<h3>Optimal Pair Matching (<code>method = &quot;optimal&quot;</code>)</h3>
<p>Optimal pair matching (often just called optimal matching) is very
similar to nearest neighbor matching in that it attempts to pair each
treated unit with one or more control units. Unlike nearest neighbor
matching, however, it is “optimal” rather than greedy; it is optimal in
the sense that it attempts to choose matches that collectively optimize
an overall criterion <span class="citation">(<a href="#ref-hansen2006">Hansen and Klopfer 2006</a>; <a href="#ref-gu1993">Gu and Rosenbaum 1993</a>)</span>. The criterion used
is the sum of the absolute pair distances in the matched sample. See
<code>?method_optimal</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;optimal&quot;</code>. Optimal
pair matching in <code>MatchIt</code> depends on the
<code>fullmatch()</code> function in the <code>optmatch</code> package
<span class="citation">(<a href="#ref-hansen2006">Hansen and Klopfer
2006</a>)</span>.</p>
<p>Like nearest neighbor matching, optimal pair matching requires the
specification of a distance measure between units. Optimal pair matching
can be thought of simply as an alternative to selecting the order of the
matching for nearest neighbor matching. Optimal pair matching and
nearest neighbor matching often yield the same or very similar matched
samples; indeed, some research has indicated that optimal pair matching
is not much better than nearest neighbor matching at yielding balanced
matched samples <span class="citation">(<a href="#ref-austin2013b">Austin 2013</a>)</span>.</p>
<p>The <code>tol</code> argument in <code>fullmatch()</code> can be
supplied to <code>matchit()</code> with <code>method = &quot;optimal&quot;</code>;
this controls the numerical tolerance used to determine whether the
optimal solution has been found. The default is fairly high and, for
smaller problems, should be set much lower (e.g., by setting
<code>tol = 1e-7</code>).</p>
</div>
<div id="optimal-full-matching-method-full" class="section level3">
<h3>Optimal Full Matching (<code>method = &quot;full&quot;</code>)</h3>
<p>Optimal full matching (often just called full matching) assigns every
treated and control unit in the sample to one subclass each <span class="citation">(<a href="#ref-hansen2004">Hansen 2004</a>; <a href="#ref-stuart2008a">Stuart and Green 2008</a>)</span>. Each subclass
contains one treated unit and one or more control units or one control
units and one or more treated units. It is optimal in the sense that the
chosen number of subclasses and the assignment of units to subclasses
minimize the sum of the absolute within-subclass distances in the
matched sample. Weights are computed based on subclass membership, and
these weights then function like propensity score weights and can be
used to estimate a weighted treatment effect, ideally free of
confounding by the measured covariates. See <code>?method_full</code>
for the documentation for <code>matchit()</code> with
<code>method = &quot;full&quot;</code>. Optimal full matching in
<code>MatchIt</code> depends on the <code>fullmatch()</code> function in
the <code>optmatch</code> package <span class="citation">(<a href="#ref-hansen2006">Hansen and Klopfer 2006</a>)</span>.</p>
<p>Like the other distance matching methods, optimal full matching
requires the specification of a distance measure between units. It can
be seen a combination of distance matching and stratum matching:
subclasses are formed with varying numbers of treated and control units,
as with stratum matching, but the subclasses are formed based on
minimizing within-pair distances and do not involve forming strata based
on any specific variable, similar to distance matching. Unlike other
distance matching methods, full matching can be used to estimate the
ATE. Full matching can also be seen as a form of propensity score
weighting that is less sensitive to the form of the propensity score
model because the original propensity scores are used just to create the
subclasses, not to form the weights directly <span class="citation">(<a href="#ref-austin2015a">Austin and Stuart 2015a</a>)</span>. In
addition, full matching does not have to rely on estimated propensity
scores to form the subclasses and weights; other distance measures are
allowed as well.</p>
<p>Although full matching uses all available units, there is a loss in
precision due to the weights. Units may be weighted in such a way that
they contribute less to the sample than would unweighted units, so the
effective sample size (ESS) of the full matching weighted sample may be
lower than even that of 1:1 pair matching. Balance is often far better
after full matching than it is with 1:k matching, making full matching a
good option to consider especially when 1:k matching is not effective or
when the ATE is the target estimand.</p>
<p>The specification of the full matching optimization problem can be
customized by supplying additional arguments that are passed to
<code>optmatch::fullmatch()</code>, such as <code>min.controls</code>,
<code>max.controls</code>, <code>mean.controls</code>, and
<code>omit.fraction</code>. As with optimal pair matching, the numerical
tolerance value can be set much lower than the default with small
problems by setting, e.g., <code>tol = 1e-7</code>.</p>
</div>
<div id="generalized-full-matching-method-quick" class="section level3">
<h3>Generalized Full Matching (<code>method = &quot;quick&quot;</code>)</h3>
<p>Generalized full matching is a variant of full matching that uses a
special fast clustering algorithm to dramatically speed up the matching,
even for large datasets <span class="citation">(<a href="#ref-savjeGeneralizedFullMatching2021">Fredrik Sävje, Higgins, and
Sekhon 2021</a>)</span>. Like with optimal full matching, generalized
full matching assigns every unit to a subclass. What makes generalized
full match “generalized” is that the user can customize the matching in
a number of ways, such as by specifying an arbitrary minimum number of
units from each treatment group or total number of units per subclass,
or by allowing not all units from a treatment group to have to be
matched. Generalized full matching minimizes the largest within-subclass
distances in the matched sample, but it does so in a way that is not
completely optimal (though the solution is often very close to the
optimal solution). Matching weights are computed based on subclass
membership, and these weights then function like propensity score
weights and can be used to estimate a weighted treatment effect, ideally
free of confounding by the measured covariates. See
<code>?method_quick</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;quick&quot;</code>. Generalized
full matching in <code>MatchIt</code> depends on the
<code>quickmatch()</code> function in the <code>quickmatch</code>
package <span class="citation">(<a href="#ref-savjeQuickmatchQuickGeneralized2018">Fredrik Sävje, Sekhon,
and Higgins 2018</a>)</span>.</p>
<p>Generalized full matching includes different options for
customization than optimal full matching. The user cannot supply their
own distance matrix, but propensity scores and distance metrics that are
computed from the supplied covariates (e.g., Mahalanobis distance) are
allowed. Calipers can only be placed on the propensity score, if
supplied. As with optimal full matching, generalized full matching can
target the ATE. Matching performance tends to be similar between the two
methods, but generalized full matching will be much quicker and can
accommodate larger datasets, making it a good substitute. Generalized
full matching is often faster than even nearest neighbor matching,
especially for large datasets.</p>
</div>
<div id="genetic-matching-method-genetic" class="section level3">
<h3>Genetic Matching (<code>method = &quot;genetic&quot;</code>)</h3>
<p>Genetic matching is less a specific form of matching and more a way
of specifying a distance measure for another form of matching. In
practice, though, the form of matching used is nearest neighbor pair
matching. Genetic matching uses a genetic algorithm, which is an
optimization routine used for non-differentiable objective functions, to
find scaling factors for each variable in a generalized Mahalanobis
distance formula <span class="citation">(<a href="#ref-diamond2013">Diamond and Sekhon 2013</a>)</span>. The
criterion optimized by the algorithm is one based on covariate balance.
Once the scaling factors have been found, nearest neighbor matching is
performed on the scaled generalized Mahalanobis distance. See
<code>?method_genetic</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;genetic&quot;</code>. Genetic
matching in <code>MatchIt</code> depends on the <code>GenMatch()</code>
function in the <code>Matching</code> package <span class="citation">(<a href="#ref-sekhon2011">Sekhon 2011</a>)</span> to perform the genetic
search and uses the <code>Match()</code> function to perform the nearest
neighbor match using the scaled generalized Mahalanobis distance.</p>
<p>Genetic matching considers the generalized Mahalanobis distance
between a treated unit <span class="math inline">\(i\)</span> and a
control unit <span class="math inline">\(j\)</span> as <span class="math display">\[\delta_{GMD}(\mathbf{x}_i,\mathbf{x}_j,
\mathbf{W})=\sqrt{(\mathbf{x}_i -
\mathbf{x}_j)&#39;(\mathbf{S}^{-1/2})&#39;\mathbf{W}(\mathbf{S}^{-1/2})(\mathbf{x}_i
- \mathbf{x}_j)}\]</span> where <span class="math inline">\(\mathbf{x}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector containing the value of
each of the <span class="math inline">\(p\)</span> included covariates
for that unit, <span class="math inline">\(\mathbf{S}^{-1/2}\)</span> is
the Cholesky decomposition of the covariance matrix <span class="math inline">\(\mathbf{S}\)</span> of the covariates, and <span class="math inline">\(\mathbf{W}\)</span> is a diagonal matrix with
scaling factors <span class="math inline">\(w\)</span> on the diagonal:
<span class="math display">\[
\mathbf{W}=\begin{bmatrix}
    w_1 &amp;  &amp; &amp; \\
     &amp; w_2 &amp; &amp; \\
     &amp;  &amp; \ddots &amp;\\
     &amp; &amp; &amp; w_p \\
    \end{bmatrix}
\]</span></p>
<p>When <span class="math inline">\(w_k=1\)</span> for all covariates
<span class="math inline">\(k\)</span>, the computed distance is the
standard Mahalanobis distance between units. Genetic matching estimates
the optimal values of the <span class="math inline">\(w_k\)</span>s,
where a user-specified criterion is used to define what is optimal. The
default is to maximize the smallest p-value among balance tests for the
covariates in the matched sample (both Kolmogorov-Smirnov tests and
t-tests for each covariate).</p>
<p>In <code>MatchIt</code>, if a propensity score is specified, the
default is to include the propensity score and the covariates in <span class="math inline">\(\mathbf{x}\)</span> and to optimize balance on the
covariates. When <code>distance = &quot;mahalanobis&quot;</code> or the
<code>mahvars</code> argument is specified, the propensity score is left
out of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>In all other respects, genetic matching functions just like nearest
neighbor matching except that the matching itself is carried out by
<code>Matching::Match()</code> instead of by <code>MatchIt</code>. When
using <code>method = &quot;genetic&quot;</code> in <code>MatchIt</code>,
additional arguments passed to <code>Matching::GenMatch()</code> to
control the genetic search process should be specified; in particular,
the <code>pop.size</code> argument should be increased from its default
of 100 to a much higher value. Doing so will make the algorithm take
more time to finish but will generally improve the quality of the
resulting matches. Different functions can be supplied to be used as the
objective in the optimization using the <code>fit.func</code>
argument.</p>
</div>
<div id="exact-matching-method-exact" class="section level3">
<h3>Exact Matching (<code>method = &quot;exact&quot;</code>)</h3>
<p>Exact matching is a form of stratum matching that involves creating
subclasses based on unique combinations of covariate values and
assigning each unit into their corresponding subclass so that only units
with identical covariate values are placed into the same subclass. Any
units that are in subclasses lacking either treated or control units
will be dropped. Exact matching is the most powerful matching method in
that no functional form assumptions are required on either the treatment
or outcome model for the method to remove confounding due to the
measured covariates; the covariate distributions are exactly balanced.
The problem with exact matching is that in general, few if any units
will remain after matching, so the estimated effect will only generalize
to a very limited population and can lack precision. Exact matching is
particularly ineffective with continuous covariates, for which it might
be that no two units have the same value, and with many covariates, for
which it might be the case that no two units have the same combination
of all covariates; this latter problem is known as the “curse of
dimensionality”. See <code>?method_exact</code> for the documentation
for <code>matchit()</code> with <code>method = &quot;exact&quot;</code>.</p>
<p>It is possible to use exact matching on some covariates and another
form of matching on the rest. This makes it possible to have exact
balance on some covariates (typically categorical) and approximate
balance on others, thereby gaining the benefits of both exact matching
and the other matching method used. To do so, the other matching method
should be specified in the <code>method</code> argument to
<code>matchit()</code> and the <code>exact</code> argument should be
specified to contain the variables on which exact matching is to be
done.</p>
</div>
<div id="coarsened-exact-matching-method-cem" class="section level3">
<h3>Coarsened Exact Matching (<code>method = &quot;cem&quot;</code>)</h3>
<p>Coarsened exact matching (CEM) is a form of stratum matching that
involves first coarsening the covariates by creating bins and then
performing exact matching on the new coarsened versions of the
covariates <span class="citation">(<a href="#ref-iacus2012">Iacus, King,
and Porro 2012</a>)</span>. The degree and method of coarsening can be
controlled by the user to manage the trade-off between exact and
approximate balancing. For example, coarsening a covariate to two bins
will mean that units that differ greatly on the covariate might be
placed into the same subclass, while coarsening a variable to five bins
may require units to be dropped due to not finding matches. Like exact
matching, CEM is susceptible to the curse of dimensionality, making it a
less viable solution with many covariates, especially with few units.
Dropping units can also change the target population of the estimated
effect. See <code>?method_cem</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;cem&quot;</code>. CEM in
<code>MatchIt</code> does not depend on any other package to perform the
coarsening and matching, though it used to rely on the <code>cem</code>
package.</p>
</div>
<div id="subclassification-method-subclass" class="section level3">
<h3>Subclassification (<code>method = &quot;subclass&quot;</code>)</h3>
<p>Propensity score subclassification can be thought of as a form of
coarsened exact matching with the propensity score as the sole covariate
to be coarsened and matched on. The bins are usually based on specified
quantiles of the propensity score distribution either in the treated
group, control group, or overall, depending on the desired estimand.
Propensity score subclassification is an old and well-studied method,
though it can perform poorly compared to other, more modern propensity
score methods such as full matching and weighting <span class="citation">(<a href="#ref-austin2010">Austin 2010a</a>)</span>.
See <code>?method_subclass</code> for the documentation for
<code>matchit()</code> with <code>method = &quot;subclass&quot;</code>.</p>
<p>The binning of the propensity scores is typically based on dividing
the distribution of covariates into approximately equally sized bins.
The user specifies the number of subclasses using the
<code>subclass</code> argument and which group should be used to compute
the boundaries of the bins using the <code>estimand</code> argument.
Sometimes, subclasses can end up with no units from one of the treatment
groups; by default, <code>matchit()</code> moves a unit from an adjacent
subclass into the lacking one to ensure that each subclass has at least
one unit from each treatment group. The minimum number of units required
in each subclass can be chosen by the <code>min.n</code> argument to
<code>matchit()</code>. If set to 0, an error will be thrown if any
subclass lacks units from one of the treatment groups. Moving units from
one subclass to another generally worsens the balance in the subclasses
but can increase precision.</p>
<p>The default number of subclasses is 6, which is arbitrary and should
not be taken as a recommended value. Although early theory has
recommended the use of 5 subclasses, in general there is an optimal
number of subclasses that is typically much larger than 5 but that
varies among datasets <span class="citation">(<a href="#ref-orihara2021">Orihara and Hamada 2021</a>)</span>. Rather than
trying to figure this out for oneself, one can use optimal full matching
(i.e., with <code>method = &quot;full&quot;</code>) or generalized full matching
(<code>method = &quot;quick&quot;</code>) to optimally create subclasses that
optimize a within-subclass distance criterion.</p>
<p>The output of propensity score subclassification includes the
assigned subclasses and the subclassification weights. Effects can be
estimated either within each subclass and then averaged across them, or
a single marginal effect can be estimated using the subclassification
weights. This latter method has been called marginal mean weighting
through subclassification [MMWS; <span class="citation">Hong (<a href="#ref-hong2010">2010</a>)</span>] and fine stratification weighting
<span class="citation">(<a href="#ref-desai2017">Desai et al.
2017</a>)</span>. It is also implemented in the <code>WeightIt</code>
package.</p>
</div>
<div id="cardinality-and-profile-matching-method-cardinality" class="section level3">
<h3>Cardinality and Profile Matching
(<code>method = &quot;cardinality&quot;</code>)</h3>
<p>Cardinality and profile matching are pure subset selection methods
that involve selecting a subset of the original sample without
considering the distance between individual units or assigning units to
pairs or subclasses. They can be thought of as a weighting method where
the weights are restricted to be zero or one. Cardinality matching
involves finding the largest sample that satisfies user-supplied balance
constraints and constraints on the ratio of matched treated to matched
control units <span class="citation">(<a href="#ref-zubizarretaMatchingBalancePairing2014">Zubizarreta, Paredes,
and Rosenbaum 2014</a>)</span>. It does not consider a specific estimand
and can be a useful alternative to matching with a caliper for handling
data with little overlap <span class="citation">(<a href="#ref-visconti2018">Visconti and Zubizarreta 2018</a>)</span>.
Profile matching involves identifying a target distribution (e.g., the
full sample for the ATE or the treated units for the ATT) and finding
the largest subset of the treated and control groups that satisfy
user-supplied balance constraints with respect to that target <span class="citation">(<a href="#ref-cohnProfileMatchingGeneralization2021">Cohn and Zubizarreta
2022</a>)</span>. See <code>?method_cardinality</code> for the
documentation for using <code>matchit()</code> with
<code>method = &quot;cardinality&quot;</code>, including which inputs are required
to request either cardinality matching or profile matching.</p>
<p>Subset selection is performed by solving a mixed integer programming
optimization problem with linear constraints. The problem involves
maximizing the size of the matched sample subject to constraints on
balance and sample size. For cardinality matching, the balance
constraints refer to the mean difference for each covariate between the
matched treated and control groups, and the sample size constraints
require the matched treated and control groups to be the same size (or
differ by a user-supplied factor). For profile matching, the balance
constraints refer to the mean difference for each covariate between each
treatment group and the target distribution; for the ATE, this requires
the mean of each covariate in each treatment group to be within a given
tolerance of the mean of the covariate in the full sample, and for the
ATT, this requires the mean of each covariate in the control group to be
within a given tolerance of the mean of the covariate in the treated
group, which is left intact. The balance tolerances are controlled by
the <code>tols</code> and <code>std.tols</code> arguments. One can also
create pairs in the matched sample by using the <code>mahvars</code>
argument, which requests that optimal Mahalanobis matching be done after
subset selection; doing so can add additional precision and robustness
<span class="citation">(<a href="#ref-zubizarretaMatchingBalancePairing2014">Zubizarreta, Paredes,
and Rosenbaum 2014</a>)</span>.</p>
<p>The optimization problem requires a special solver to solve.
Currently, the available options in <code>MatchIt</code> are the HiGHS
solver (through the <code>highs</code> package), the GLPK solver
(through the <code>Rglpk</code> package), the SYMPHONY solver (through
the <code>Rsymphony</code> package), and the Gurobi solver (through the
<code>gurobi</code> package). The differences among the solvers are in
performance; Gurobi is by far the best (fastest, least likely to fail to
find a solution), but it is proprietary (though has a free trial and
academic license) and is a bit more complicated to install. HiGHS is the
default due to being open source, easily installed, and with performance
comparable to Gurobi. The <code>designmatch</code> package also provides
an implementation of cardinality matching with more options than
<code>MatchIt</code> offers.</p>
</div>
</div>
<div id="customizing-the-matching-specification" class="section level2">
<h2>Customizing the Matching Specification</h2>
<p>In addition to the specific matching method, other options are
available for many of the matching methods to further customize the
matching specification. These include different specifications of the
distance measure, methods to perform alternate forms of matching in
addition to the main method, prune units far from other units prior to
matching, restrict possible matches, etc. Not all options are compatible
with all matching methods.</p>
<div id="specifying-the-propensity-score-or-other-distance-measure-distance" class="section level3">
<h3>Specifying the propensity score or other distance measure
(<code>distance</code>)</h3>
<p>The distance measure is used to define how close two units are. In
nearest neighbor matching, this is used to choose the nearest control
unit to each treated unit. In optimal matching, this is used in the
criterion that is optimized. By default, the distance measure is the
propensity score difference, and the argument supplied to
<code>distance</code> corresponds to the method of estimating the
propensity score. In <code>MatchIt</code>, propensity scores are often
labeled as “distance” values, even though the propensity score itself is
not a distance measure. This is to reflect that the propensity score is
used in creating the distance value, but other scores could be used,
such as prognostic scores for prognostic score matching <span class="citation">(<a href="#ref-hansen2008a">Hansen 2008</a>)</span>.
The propensity score is more like a “position” value, in that it
reflects the position of each unit in the matching space, and the
difference between positions is the distance between them. If the
argument to <code>distance</code> is one of the allowed methods for
estimating propensity scores (see <code>?distance</code> for these
values) or is a numeric vector with one value per unit, the distance
between units will be computed as the pairwise difference between
propensity scores or the supplied values. Propensity scores are also
used in propensity score subclassification and can optionally be used in
genetic matching as a component of the generalized Mahalanobis distance.
For exact, coarsened exact, and cardinality matching, the
<code>distance</code> argument is ignored.</p>
<p>The default <code>distance</code> argument is <code>&quot;glm&quot;</code>,
which estimates propensity scores using logistic regression or another
generalized linear model. The <code>link</code> and
<code>distance.options</code> arguments can be supplied to further
specify the options for the propensity score models, including whether
to use the raw propensity score or a linearized version of it (e.g., the
logit of a logistic regression propensity score, which has been commonly
referred to and recommended in the propensity score literature <span class="citation">(<a href="#ref-austin2011a">Austin 2011</a>; <a href="#ref-stuart2010">Stuart 2010</a>)</span>). Allowable options for
the propensity score model include parametric and machine learning-based
models, each of which have their strengths and limitations and may
perform differently depending on the unique qualities of each dataset.
We recommend multiple types of models be tried to find one that yields
the best balance, as there is no way to make a single recommendation
that will work for all cases.</p>
<p>The <code>distance</code> argument can also be specified as a method
of computing pairwise distances from the covariates directly (i.e.,
without estimating propensity scores). The options include
<code>&quot;mahalanobis&quot;</code>, <code>&quot;robust_mahalanobis&quot;</code>,
<code>&quot;euclidean&quot;</code>, and <code>&quot;scaled_euclidean&quot;</code>. These
methods compute a distance metric for a treated unit <span class="math inline">\(i\)</span> and a control unit <span class="math inline">\(j\)</span> as <span class="math display">\[\delta(\mathbf{x}_i,\mathbf{x}_j)=\sqrt{(\mathbf{x}_i
- \mathbf{x}_j)&#39;S^{-1}(\mathbf{x}_i - \mathbf{x}_j)}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is a <span class="math inline">\(p \times 1\)</span> vector containing the value of
each of the <span class="math inline">\(p\)</span> included covariates
for that unit, <span class="math inline">\(S\)</span> is a scaling
matrix, and <span class="math inline">\(S^{-1}\)</span> is the
(generalized) inverse of <span class="math inline">\(S\)</span>. For
Mahalanobis distance matching, <span class="math inline">\(S\)</span> is
the pooled covariance matrix of the covariates <span class="citation">(<a href="#ref-rubinBiasReductionUsing1980">Rubin
1980</a>)</span>; for Euclidean distance matching, <span class="math inline">\(S\)</span> is the identity matrix (i.e., no
scaling); and for scaled Euclidean distance matching, <span class="math inline">\(S\)</span> is the diagonal of the pooled
covariance matrix (containing just the variances). The robust
Mahalanobis distance is computed not on the covariates directly but
rather on their ranks and uses a correction for ties (see <span class="citation">Rosenbaum (<a href="#ref-rosenbaumDesignObservationalStudies2010">2010</a>)</span>, ch
8). For creating close pairs, matching with these distance measures
tends work better than propensity score matching because paired units
will have close values on all of the covariates, whereas propensity
score-paired units may be close on the propensity score but not on any
of the covariates themselves. This feature was the basis of King and
Nielsen’s <span class="citation">(<a href="#ref-king2019">2019</a>)</span> warning against using propensity
scores for matching. That said, they do not always outperform propensity
score matching <span class="citation">(<a href="#ref-ripolloneImplicationsPropensityScore2018">Ripollone et al.
2018</a>)</span>.</p>
<p><code>distance</code> can also be supplied as a matrix of distance
values between units. This makes it possible to use handcrafted distance
matrices or distances created outside <code>MatchIt</code>. Only nearest
neighbor, optimal pair, and optimal full matching allow this
specification.</p>
<p>The propensity score can have uses other than as the basis for
matching. It can be used to define a region of common support, outside
which units are dropped prior to matching; this is implemented by the
<code>discard</code> option. It can also be used to define a caliper,
the maximum distance two units can be before they are prohibited from
being paired with each other; this is implemented by the
<code>caliper</code> argument. To estimate or supply a propensity score
for one of these purposes but not use it as the distance measure for
matching (i.e., to perform Mahalanobis distance matching instead), the
<code>mahvars</code> argument can be specified. These options are
described below.</p>
</div>
<div id="implementing-common-support-restrictions-discard" class="section level3">
<h3>Implementing common support restrictions (<code>discard</code>)</h3>
<p>The region of <em>common support</em> is the region of overlap
between treatment groups. A common support restriction discards units
that fall outside of the region of common support, preventing them from
being matched to other units and included in the matched sample. This
can reduce the potential for extrapolation and help the matching
algorithms to avoid overly distant matches from occurring. In
<code>MatchIt</code>, the <code>discard</code> option implements a
common support restriction based on the propensity score. The argument
can be supplied as <code>&quot;treated&quot;</code>, <code>&quot;control&quot;</code>, or
<code>&quot;both&quot;</code>, which discards units in the corresponding group
that fall outside the region of common support for the propensity score.
The <code>reestimate</code> argument can be supplied to choose whether
to re-estimate the propensity score in the remaining units. <strong>If
units from the treated group are discarded based on a common support
restriction, the estimand no longer corresponds to the ATT.</strong></p>
</div>
<div id="caliper-matching-caliper" class="section level3">
<h3>Caliper matching (<code>caliper</code>)</h3>
<p>A <em>caliper</em> can be though of as a ring around each unit that
limits to which other units that unit can be paired. Calipers are based
on the propensity score or other covariates. Two units whose distance on
a calipered covariate is larger than the caliper width for that
covariate are not allowed to be matched to each other. Any units for
which there are no available matches within the caliper are dropped from
the matched sample. Calipers ensure paired units are close to each other
on the calipered covariates, which can ensure good balance in the
matched sample. Multiple variables can be supplied to
<code>caliper</code> to enforce calipers on all of them simultaneously.
Using calipers can be a good alternative to exact or coarsened exact
matching to ensure only similar units are paired with each other. The
<code>std.caliper</code> argument controls whether the provided calipers
are in raw units or standard deviation units. When negative calipers are
supplied, this forces units whose distance on the calipered covariate is
<em>smaller</em> than the absolute caliper width for that covariate to
be disallowed from being matched to each other. <strong>If units from
the treated group are left unmatched due to a caliper, the estimand no
longer corresponds to the ATT.</strong></p>
</div>
<div id="mahalanobis-distance-matching-mahvars" class="section level3">
<h3>Mahalanobis distance matching (<code>mahvars</code>)</h3>
<p>To perform Mahalanobis distance matching without the need to estimate
or use a propensity score, the <code>distance</code> argument can be set
to <code>&quot;mahalanobis&quot;</code>. If a propensity score is to be estimated
or used for a different purpose, such as in a common support restriction
or a caliper, but you still want to perform Mahalanobis distance
matching, variables should be supplied to the <code>mahvars</code>
argument. The propensity scores will be generated using the
<code>distance</code> specification, and matching will occur not on the
covariates supplied to the main formula of <code>matchit()</code> but
rather on the covariates supplied to <code>mahvars</code>. To perform
Mahalanobis distance matching within a propensity score caliper, for
example, the <code>distance</code> argument should be set to the method
of estimating the propensity score (e.g., <code>&quot;glm&quot;</code> for
logistic regression), the <code>caliper</code> argument should be
specified to the desired caliper width, and <code>mahvars</code> should
be specified to perform Mahalanobis distance matching on the desired
covariates within the caliper. <code>mahvars</code> has a special
meaning for genetic matching and cardinality matching; see their
respective help pages for details.</p>
</div>
<div id="exact-matching-exact" class="section level3">
<h3>Exact matching (<code>exact</code>)</h3>
<p>To perform exact matching on all supplied covariates, the
<code>method</code> argument can be set to <code>&quot;exact&quot;</code>. To
perform exact matching only on some covariates and some other form of
matching within exact matching strata on other covariates, the
<code>exact</code> argument can be used. Covariates supplied to the
<code>exact</code> argument will be matched exactly, and the form of
matching specified by <code>method</code> (e.g., <code>&quot;nearest&quot;</code>
for nearest neighbor matching) will take place within each exact
matching stratum. This can be a good way to gain some of the benefits of
exact matching without completely succumbing to the curse of
dimensionality. As with exact matching performed with
<code>method = &quot;exact&quot;</code>, any units in strata lacking members of
one of the treatment groups will be left unmatched. Note that although
matching occurs within each exact matching stratum, propensity score
estimation and computation of the Mahalanobis or other distance matrix
occur in the full sample. <strong>If units from the treated group are
unmatched due to an exact matching restriction, the estimand no longer
corresponds to the ATT.</strong></p>
</div>
<div id="anti-exact-matching-antiexact" class="section level3">
<h3>Anti-exact matching (<code>antiexact</code>)</h3>
<p>Anti-exact matching adds a restriction such that a treated and
control unit with same values of any of the specified anti-exact
matching variables cannot be paired. This can be useful when finding
comparison units outside of a unit’s group, such as when matching units
in one group to units in another when units within the same group might
otherwise be close matches. See examples <a href="https://stackoverflow.com/q/66526115/6348551">here</a> and <a href="https://stackoverflow.com/q/61120201/6348551">here</a>. A similar
effect can be implemented by supplying negative caliper values.</p>
</div>
<div id="matching-with-replacement-replace" class="section level3">
<h3>Matching with replacement (<code>replace</code>)</h3>
<p>Nearest neighbor matching and genetic matching have the option of
matching with or without replacement, and this is controlled by the
<code>replace</code> argument. Matching without replacement means that
each control unit is matched to only one treated unit, while matching
with replacement means that control units can be reused and matched to
multiple treated units. Matching without replacement carries certain
statistical benefits in that weights for each unit can be omitted or are
more straightforward to include and dependence between units depends
only on pair membership. However, it is not asymptotically consistent
unless the propensity scores for all treated units are below .5 and
there are many more control units than treated units <span class="citation">(<a href="#ref-savjeInconsistencyMatchingReplacement2022">F. Sävje
2022</a>)</span>. Special standard error estimators are sometimes
required for estimating effects after matching with replacement <span class="citation">(<a href="#ref-austin2020a">Austin and Cafri
2020</a>)</span>, and methods for accounting for uncertainty are not
well understood for non-continuous outcomes. Matching with replacement
will tend to yield better balance though, because the problem of
“running out” of close control units to match to treated units is
avoided, though the reuse of control units will decrease the effect
sample size, thereby worsening precision <span class="citation">(<a href="#ref-austin2013b">Austin 2013</a>)</span>. (This problem occurs in
the Lalonde dataset used in <code>vignette(&quot;MatchIt&quot;)</code>, which is
why nearest neighbor matching without replacement is not very effective
there.) After matching with replacement, control units are assigned to
more than one subclass, so the <code>get_matches()</code> function
should be used instead of <code>match_data()</code> after matching with
replacement if subclasses are to be used in follow-up analyses; see
<code>vignette(&quot;estimating-effects&quot;)</code> for details.</p>
<p>The <code>reuse.max</code> argument can also be used with
<code>method = &quot;nearest&quot;</code> to control how many times each control
unit can be reused as a match. Setting <code>reuse.max = 1</code> is
equivalent to requiring matching without replacement (i.e., because each
control can be used only once). Other values allow control units to be
matched more than once, though only up to the specified number of times.
Higher values will tend to improve balance at the cost of precision.</p>
</div>
<div id="k1-matching-ratio" class="section level3">
<h3><span class="math inline">\(k\)</span>:1 matching
(<code>ratio</code>)</h3>
<p>The most common form of matching, 1:1 matching, involves pairing one
control unit with each treated unit. To perform <span class="math inline">\(k\)</span>:1 matching (e.g., 2:1 or 3:1), which
pairs (up to) <span class="math inline">\(k\)</span> control units with
each treated unit, the <code>ratio</code> argument can be specified.
Performing <span class="math inline">\(k\)</span>:1 matching can
preserve precision by preventing too many control units from being
unmatched and dropped from the matched sample, though the gain in
precision by increasing <span class="math inline">\(k\)</span>
diminishes rapidly after 4 <span class="citation">(<a href="#ref-rosenbaum2020">Rosenbaum 2020</a>)</span>. Importantly, for
<span class="math inline">\(k&gt;1\)</span>, the matches after the first
match will generally be worse than the first match in terms of closeness
to the treated unit, so increasing <span class="math inline">\(k\)</span> can also worsen balance <span class="citation">(<a href="#ref-rassenOnetomanyPropensityScore2012">Rassen et al.
2012</a>)</span>. <span class="citation">Austin (<a href="#ref-austin2010a">2010b</a>)</span> found that 1:1 or 1:2 matching
generally performed best in terms of mean squared error. In general, it
makes sense to use higher values of <span class="math inline">\(k\)</span> while ensuring that balance is
satisfactory.</p>
<p>With nearest neighbor and optimal pair matching, variable <span class="math inline">\(k\)</span>:1 matching, in which the number of
controls matched to each treated unit varies, can also be used; this can
have improved performance over “fixed” <span class="math inline">\(k\)</span>:1 matching <span class="citation">(<a href="#ref-ming2000">Ming and Rosenbaum 2000</a>; <a href="#ref-rassenOnetomanyPropensityScore2012">Rassen et al.
2012</a>)</span>. See <code>?method_nearest</code> and
<code>?method_optimal</code> for information on implementing variable
<span class="math inline">\(k\)</span>:1 matching.</p>
</div>
<div id="matching-order-m.order" class="section level3">
<h3>Matching order (<code>m.order</code>)</h3>
<p>For nearest neighbor matching (including genetic matching), units are
matched in an order, and that order can affect the quality of individual
matches and of the resulting matched sample. With
<code>method = &quot;nearest&quot;</code>, the allowable options to
<code>m.order</code> to control the matching order are
<code>&quot;largest&quot;</code>, <code>&quot;smallest&quot;</code>, <code>&quot;closest&quot;</code>,
<code>&quot;farthest&quot;</code>, <code>&quot;random&quot;</code>, and <code>&quot;data&quot;</code>.
With <code>method = &quot;genetic&quot;</code>, all but <code>&quot;closest&quot;</code> and
<code>&quot;farthest&quot;</code> can be used. Requesting <code>&quot;largest&quot;</code>
means that treated units with the largest propensity scores, i.e., those
least like the control units, will be matched first, which prevents them
from having bad matches after all the close control units have been used
up. <code>&quot;smallest&quot;</code> means that treated units with the smallest
propensity scores are matched first. <code>&quot;closest&quot;</code> means that
potential pairs with the smallest distance between units will be matched
first, which ensures that the best possible matches are included in the
matched sample but can yield poor matches for units whose best match is
far from them; this makes it particularly useful when matching with a
caliper. <code>&quot;farthest&quot;</code> means that closest pairs with the
largest distance between them will be matched first, which ensures the
hardest units to match are given the best chance to find matches.
<code>&quot;random&quot;</code> matches in a random order, and <code>&quot;data&quot;</code>
matches in order of the data. A propensity score is required for
<code>&quot;largest&quot;</code> and <code>&quot;smallest&quot;</code> but not for the other
options.</p>
<p><span class="citation">Rubin (<a href="#ref-rubin1973">1973</a>)</span> recommends using
<code>&quot;largest&quot;</code> or <code>&quot;random&quot;</code>, though <span class="citation">Austin (<a href="#ref-austin2013b">2013</a>)</span>
recommends against <code>&quot;largest&quot;</code> and instead favors
<code>&quot;closest&quot;</code> or <code>&quot;random&quot;</code>. <code>&quot;closest&quot;</code>
and <code>&quot;smallest&quot;</code> are best for prioritizing the best possible
matches, while <code>&quot;farthest&quot;</code> and <code>&quot;largest&quot;</code> are
best for preventing extreme pairwise distances between matched
units.</p>
</div>
</div>
<div id="choosing-a-matching-method" class="section level2">
<h2>Choosing a Matching Method</h2>
<p>Choosing the best matching method for one’s data depends on the
unique characteristics of the dataset as well as the goals of the
analysis. For example, because different matching methods can target
different estimands, when certain estimands are desired, specific
methods must be used. On the other hand, some methods may be more
effective than others when retaining the target estimand is less
important. Below we provide some guidance on choosing a matching method.
Remember that multiple methods can (and should) be tried as long as the
treatment effect is not estimated until a method has been settled
on.</p>
<p>The criteria on which a matching specification should be judged are
balance and remaining (effective) sample size after matching. Assessing
balance is described in <code>vignette(&quot;assessing-balance&quot;)</code>. A
typical workflow is similar to that demonstrated in
<code>vignette(&quot;MatchIt&quot;)</code>: try a matching method, and if it
yields poor balance or an unacceptably low remaining sample size, try
another, until a satisfactory specification has been found. It is
important to assess balance broadly (i.e., beyond comparing the means of
the covariates in the treated and control groups), and the search for a
matching specification should not stop when a threshold is reached, but
should attempt to come as close as possible to perfect balance <span class="citation">(<a href="#ref-ho2007">Ho et al. 2007</a>)</span>. Even
if the first matching specification appears successful at reducing
imbalance, there may be another specification that could reduce it even
further, thereby increasing the robustness of the inference and the
plausibility of an unbiased effect estimate.</p>
<p>If the target of inference is the ATE, optimal or generalized full
matching, subclassification, or profile matching can be used. If the
target of inference is the ATT or ATC, any matching method may be used.
When retaining the target estimand is not so important, additional
options become available that involve discarding units in such a way
that the original estimand is distorted. These include matching with a
caliper, matching within a region of common support, cardinality
matching, or exact or coarsened exact matching, perhaps on a subset of
the covariates.</p>
<p>Because exact and coarsened exact matching aim to balance the entire
joint distribution of covariates, they are the most powerful methods. If
it is possible to perform exact matching, this method should be used. If
continuous covariates are present, coarsened exact matching can be
tried. Care should be taken with retaining the target population and
ensuring enough matched units remain; unless the control pool is much
larger than the treated pool, it is likely some (or many) treated units
will be discarded, thereby changing the estimand and possibly
dramatically reducing precision. These methods are typically only
available in the most optimistic of circumstances, but they should be
used first when those circumstances arise. It may also be useful to
combine exact or coarsened exact matching on some covariates with
another form of matching on the others (i.e., by using the
<code>exact</code> argument).</p>
<p>When estimating the ATE, either subclassification, full matching, or
profile matching can be used. Optimal and generalized full matching can
be effective because they optimize a balance criterion, often leading to
better balance. With full matching, it’s also possible to exact match on
some variables and match using the Mahalanobis distance, eliminating the
need to estimate propensity scores. Profile matching also ensures good
balance, but because units are only given weights of zero or one, a
solution may not be feasible and many units may have to be discarded.
For large datasets, neither optimal full matching nor profile matching
may be possible, in which case generalized full matching and
subclassification are faster solutions. When using subclassification,
the number of subclasses should be varied. With large samples, higher
numbers of subclasses tend to yield better performance; one should not
immediately settle for the default (6) or the often-cited recommendation
of 5 without trying several other numbers. The documentation for
<code>cobalt::bal.compute()</code> contains an example of using balance
to select the optimal number of subclasses.</p>
<p>When estimating the ATT, a variety of methods can be tried. Genetic
matching can perform well at achieving good balance because it directly
optimizes covariate balance. With larger datasets, it may take a long
time to reach a good solution (though that solution will tend to be good
as well). Profile matching also will achieve good balance if a solution
is feasible because balance is controlled by the user. Optimal pair
matching and nearest neighbor matching without replacement tend to
perform similarly to each other; nearest neighbor matching may be
preferable for large datasets that cannot be handled by optimal
matching. Nearest neighbor, optimal, and genetic matching allow some
customizations like including covariates on which to exactly match,
using the Mahalanobis distance instead of a propensity score difference,
and performing <span class="math inline">\(k\)</span>:1 matching with
<span class="math inline">\(k&gt;1\)</span>. Nearest neighbor matching
with replacement, full matching, and subclassification all involve
weighting the control units with nonuniform weights, which often allows
for improved balancing capabilities but can be accompanied by a loss in
effective sample size, even when all units are retained. There is no
reason not to try many of these methods, varying parameters here and
there, in search of good balance and high remaining sample size. As
previously mentioned, no single method can be recommended above all
others because the optimal specification depends on the unique qualities
of each dataset.</p>
<p>When the target population is less important, for example, when
engaging in treatment effect discovery or when the sampled population is
not of particular interest (e.g., it corresponds to an arbitrarily
chosen hospital or school; see <span class="citation">Mao, Li, and
Greene (<a href="#ref-mao2018">2018</a>)</span> for these and other
reasons why retaining the target population may not be important), other
methods that do not retain the characteristics of the original sample
become available. These include matching with a caliper (on the
propensity score or on the covariates themselves), cardinality matching,
and more restrictive forms of matching like exact and coarsened exact
matching, either on all covariates or just a subset, that are prone to
discard units from the sample in such a way that the target population
is changed. <span class="citation">Austin (<a href="#ref-austin2013b">2013</a>)</span> and Austin and Stuart <span class="citation">(<a href="#ref-austin2015c">2015b</a>, <a href="#ref-austin2015a">2015a</a>)</span> have found that caliper
matching can be a particularly effective modification to nearest
neighbor matching for eliminating imbalance and reducing bias when the
target population is less relevant, but when inference to a specific
target population is desired, using calipers can induce bias due to
incomplete matching <span class="citation">(<a href="#ref-rosenbaum1985">Rosenbaum and Rubin 1985a</a>; <a href="#ref-wang2020">Wang 2020</a>)</span>. Cardinality matching can be
particularly effective in data with little overlap between the treatment
groups <span class="citation">(<a href="#ref-visconti2018">Visconti and
Zubizarreta 2018</a>)</span> and can perform better than caliper
matching <span class="citation">(<a href="#ref-delosangelesresaDirectStableWeight2020">de los Angeles Resa
and Zubizarreta 2020</a>)</span>.</p>
<p>It is important not to rely excessively on theoretical or
simulation-based findings or specific recommendations when making
choices about the best matching method to use. For example, although
nearest neighbor matching without replacement balance covariates better
than did subclassification with five or ten subclasses in Austin’s <span class="citation">(<a href="#ref-austin2009c">2009</a>)</span>
simulation, this does not imply it will be superior in all datasets.
Likewise, though <span class="citation">Rosenbaum and Rubin (<a href="#ref-rosenbaum1985a">1985b</a>)</span> and <span class="citation">Austin (<a href="#ref-austin2011a">2011</a>)</span>
both recommend using a caliper of .2 standard deviations of the logit of
the propensity score, this does not imply that caliper will be optimal
in all scenarios, and other widths should be tried, though it should be
noted that tightening the caliper on the propensity score can sometimes
degrade performance <span class="citation">(<a href="#ref-king2019">King
and Nielsen 2019</a>)</span>.</p>
<p>For large datasets (i.e., in 10,000s to millions), some matching
methods will be too slow to be used at scale. Instead, users should
consider generalized full matching, subclassification, or coarsened
exact matching, which are all very fast and designed to work with large
datasets. Nearest neighbor matching on the propensity score has been
optimized to run quickly for large datasets as well.</p>
</div>
<div id="reporting-the-matching-specification" class="section level2">
<h2>Reporting the Matching Specification</h2>
<p>When reporting the results of a matching analysis, it is important to
include the relevant details of the final matching specification and the
process of arriving at it. Using <code>print()</code> on the
<code>matchit</code> object synthesizes information on how the above
arguments were used to provide a description of the matching
specification. It is best to be as specific as possible to ensure the
analysis is replicable and to allow audiences to assess its validity.
Although citations recommending specific matching methods can be used to
help justify a choice, the only sufficient justification is adequate
balance and remaining sample size, regardless of published
recommendations for specific methods. See
<code>vignette(&quot;assessing-balance&quot;)</code> for instructions on how to
assess and report the quality of a matching specification. After
matching and estimating an effect, details of the effect estimation must
be included as well; see <code>vignette(&quot;estimating-effects&quot;)</code> for
instructions on how to perform and report on the analysis of a matched
dataset.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-abadie2006" class="csl-entry">
Abadie, Alberto, and Guido W. Imbens. 2006. <span>“Large Sample
Properties of Matching Estimators for Average Treatment Effects.”</span>
<em>Econometrica</em> 74 (1): 235–67. <a href="https://doi.org/10.1111/j.1468-0262.2006.00655.x">https://doi.org/10.1111/j.1468-0262.2006.00655.x</a>.
</div>
<div id="ref-abadie2016" class="csl-entry">
———. 2016. <span>“Matching on the Estimated Propensity Score.”</span>
<em>Econometrica</em> 84 (2): 781–807. <a href="https://doi.org/10.3982/ECTA11293">https://doi.org/10.3982/ECTA11293</a>.
</div>
<div id="ref-austin2009c" class="csl-entry">
Austin, Peter C. 2009. <span>“The Relative Ability of Different
Propensity Score Methods to Balance Measured Covariates Between Treated
and Untreated Subjects in Observational Studies.”</span> <em>Medical
Decision Making</em> 29 (6): 661–77. <a href="https://doi.org/10.1177/0272989x09341755">https://doi.org/10.1177/0272989x09341755</a>.
</div>
<div id="ref-austin2010" class="csl-entry">
———. 2010a. <span>“The Performance of Different Propensity-Score Methods
for Estimating Differences in Proportions (Risk Differences or Absolute
Risk Reductions) in Observational Studies.”</span> <em>Statistics in
Medicine</em> 29 (20): 2137–48. <a href="https://doi.org/10.1002/sim.3854">https://doi.org/10.1002/sim.3854</a>.
</div>
<div id="ref-austin2010a" class="csl-entry">
———. 2010b. <span>“Statistical Criteria for Selecting the Optimal Number
of Untreated Subjects Matched to Each Treated Subject When Using
Many-to-One Matching on the Propensity Score.”</span> <em>American
Journal of Epidemiology</em> 172 (9): 1092–97. <a href="https://doi.org/10.1093/aje/kwq224">https://doi.org/10.1093/aje/kwq224</a>.
</div>
<div id="ref-austin2011a" class="csl-entry">
———. 2011. <span>“Optimal Caliper Widths for
Propensity<span>-</span>Score Matching When Estimating Differences in
Means and Differences in Proportions in Observational Studies.”</span>
<em>Pharmaceutical Statistics</em> 10 (2): 150–61. <a href="https://doi.org/10.1002/pst.433">https://doi.org/10.1002/pst.433</a>.
</div>
<div id="ref-austin2013b" class="csl-entry">
———. 2013. <span>“A Comparison of 12 Algorithms for Matching on the
Propensity Score.”</span> <em>Statistics in Medicine</em> 33 (6):
1057–69. <a href="https://doi.org/10.1002/sim.6004">https://doi.org/10.1002/sim.6004</a>.
</div>
<div id="ref-austin2020a" class="csl-entry">
Austin, Peter C., and Guy Cafri. 2020. <span>“Variance Estimation When
Using Propensity<span>-</span>Score Matching with Replacement with
Survival or Time<span>-</span>to<span>-</span>Event Outcomes.”</span>
<em>Statistics in Medicine</em> 39 (11): 1623–40. <a href="https://doi.org/10.1002/sim.8502">https://doi.org/10.1002/sim.8502</a>.
</div>
<div id="ref-austin2014a" class="csl-entry">
Austin, Peter C., and Dylan S. Small. 2014. <span>“The Use of
Bootstrapping When Using Propensity<span>-</span>Score Matching Without
Replacement: A Simulation Study.”</span> <em>Statistics in Medicine</em>
33 (24): 4306–19. <a href="https://doi.org/10.1002/sim.6276">https://doi.org/10.1002/sim.6276</a>.
</div>
<div id="ref-austin2015a" class="csl-entry">
Austin, Peter C., and Elizabeth A. Stuart. 2015a. <span>“The Performance
of Inverse Probability of Treatment Weighting and Full Matching on the
Propensity Score in the Presence of Model Misspecification When
Estimating the Effect of Treatment on Survival Outcomes.”</span>
<em>Statistical Methods in Medical Research</em> 26 (4): 1654–70. <a href="https://doi.org/10.1177/0962280215584401">https://doi.org/10.1177/0962280215584401</a>.
</div>
<div id="ref-austin2015c" class="csl-entry">
———. 2015b. <span>“Estimating the Effect of Treatment on Binary Outcomes
Using Full Matching on the Propensity Score.”</span> <em>Statistical
Methods in Medical Research</em> 26 (6): 2505–25. <a href="https://doi.org/10.1177/0962280215601134">https://doi.org/10.1177/0962280215601134</a>.
</div>
<div id="ref-cohnProfileMatchingGeneralization2021" class="csl-entry">
Cohn, Eric R., and José R. Zubizarreta. 2022. <span>“Profile
<span>Matching</span> for the <span>Generalization</span> and
<span>Personalization</span> of <span>Causal</span>
<span>Inferences</span>.”</span> <em>Epidemiology</em> 33 (5): 678. <a href="https://doi.org/10.1097/EDE.0000000000001517">https://doi.org/10.1097/EDE.0000000000001517</a>.
</div>
<div id="ref-delosangelesresaDirectStableWeight2020" class="csl-entry">
de los Angeles Resa, María, and José R. Zubizarreta. 2020. <span>“Direct
and Stable Weight Adjustment in Non-Experimental Studies with
Multivalued Treatments: Analysis of the Effect of an Earthquake on
Post-Traumatic Stress.”</span> <em>Journal of the Royal Statistical
Society: Series A (Statistics in Society)</em> n/a (n/a). <a href="https://doi.org/10.1111/rssa.12561">https://doi.org/10.1111/rssa.12561</a>.
</div>
<div id="ref-desai2017" class="csl-entry">
Desai, Rishi J., Kenneth J. Rothman, Brian T. Bateman, Sonia
Hernandez-Diaz, and Krista F. Huybrechts. 2017. <span>“A
Propensity-Score-Based Fine Stratification Approach for Confounding
Adjustment When Exposure Is Infrequent:”</span> <em>Epidemiology</em> 28
(2): 249–57. <a href="https://doi.org/10.1097/EDE.0000000000000595">https://doi.org/10.1097/EDE.0000000000000595</a>.
</div>
<div id="ref-diamond2013" class="csl-entry">
Diamond, Alexis, and Jasjeet S. Sekhon. 2013. <span>“Genetic Matching
for Estimating Causal Effects: A General Multivariate Matching Method
for Achieving Balance in Observational Studies.”</span> <em>Review of
Economics and Statistics</em> 95 (3): 932945. <a href="https://doi.org/10.1162/REST_a_00318">https://doi.org/10.1162/REST_a_00318</a>.
</div>
<div id="ref-gu1993" class="csl-entry">
Gu, Xing Sam, and Paul R. Rosenbaum. 1993. <span>“Comparison of
Multivariate Matching Methods: Structures, Distances, and
Algorithms.”</span> <em>Journal of Computational and Graphical
Statistics</em> 2 (4): 405. <a href="https://doi.org/10.2307/1390693">https://doi.org/10.2307/1390693</a>.
</div>
<div id="ref-hansen2004" class="csl-entry">
Hansen, Ben B. 2004. <span>“Full Matching in an Observational Study of
Coaching for the SAT.”</span> <em>Journal of the American Statistical
Association</em> 99 (467): 609–18. <a href="https://doi.org/10.1198/016214504000000647">https://doi.org/10.1198/016214504000000647</a>.
</div>
<div id="ref-hansen2008a" class="csl-entry">
———. 2008. <span>“The Prognostic Analogue of the Propensity
Score.”</span> <em>Biometrika</em> 95 (2): 481–88. <a href="https://doi.org/10.1093/biomet/asn004">https://doi.org/10.1093/biomet/asn004</a>.
</div>
<div id="ref-hansen2006" class="csl-entry">
Hansen, Ben B., and Stephanie O. Klopfer. 2006. <span>“Optimal Full
Matching and Related Designs via Network Flows.”</span> <em>Journal of
Computational and Graphical Statistics</em> 15 (3): 609–27. <a href="https://doi.org/10.1198/106186006X137047">https://doi.org/10.1198/106186006X137047</a>.
</div>
<div id="ref-ho2007" class="csl-entry">
Ho, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007.
<span>“Matching as Nonparametric Preprocessing for Reducing Model
Dependence in Parametric Causal Inference.”</span> <em>Political
Analysis</em> 15 (3): 199–236. <a href="https://doi.org/10.1093/pan/mpl013">https://doi.org/10.1093/pan/mpl013</a>.
</div>
<div id="ref-hong2010" class="csl-entry">
Hong, Guanglei. 2010. <span>“Marginal Mean Weighting Through
Stratification: Adjustment for Selection Bias in Multilevel
Data.”</span> <em>Journal of Educational and Behavioral Statistics</em>
35 (5): 499–531. <a href="https://doi.org/10.3102/1076998609359785">https://doi.org/10.3102/1076998609359785</a>.
</div>
<div id="ref-iacus2012" class="csl-entry">
Iacus, Stefano M., Gary King, and Giuseppe Porro. 2012. <span>“Causal
Inference Without Balance Checking: Coarsened Exact Matching.”</span>
<em>Political Analysis</em> 20 (1): 1–24. <a href="https://doi.org/10.1093/pan/mpr013">https://doi.org/10.1093/pan/mpr013</a>.
</div>
<div id="ref-king2019" class="csl-entry">
King, Gary, and Richard Nielsen. 2019. <span>“Why Propensity Scores
Should Not Be Used for Matching.”</span> <em>Political Analysis</em>,
May, 1–20. <a href="https://doi.org/10.1017/pan.2019.11">https://doi.org/10.1017/pan.2019.11</a>.
</div>
<div id="ref-mao2018" class="csl-entry">
Mao, Huzhang, Liang Li, and Tom Greene. 2018. <span>“Propensity Score
Weighting Analysis and Treatment Effect Discovery.”</span>
<em>Statistical Methods in Medical Research</em>, June, 096228021878117.
<a href="https://doi.org/10.1177/0962280218781171">https://doi.org/10.1177/0962280218781171</a>.
</div>
<div id="ref-ming2000" class="csl-entry">
Ming, Kewei, and Paul R. Rosenbaum. 2000. <span>“Substantial Gains in
Bias Reduction from Matching with a Variable Number of Controls.”</span>
<em>Biometrics</em> 56 (1): 118–24. <a href="https://doi.org/10.1111/j.0006-341X.2000.00118.x">https://doi.org/10.1111/j.0006-341X.2000.00118.x</a>.
</div>
<div id="ref-orihara2021" class="csl-entry">
Orihara, Shunichiro, and Etsuo Hamada. 2021. <span>“Determination of the
Optimal Number of Strata for Propensity Score Subclassification.”</span>
<em>Statistics &amp; Probability Letters</em> 168 (January): 108951. <a href="https://doi.org/10.1016/j.spl.2020.108951">https://doi.org/10.1016/j.spl.2020.108951</a>.
</div>
<div id="ref-rassenOnetomanyPropensityScore2012" class="csl-entry">
Rassen, Jeremy A., Abhi A. Shelat, Jessica Myers, Robert J. Glynn,
Kenneth J. Rothman, and Sebastian Schneeweiss. 2012. <span>“One-to-Many
Propensity Score Matching in Cohort Studies.”</span>
<em>Pharmacoepidemiology and Drug Safety</em> 21 (S2): 69–80. <a href="https://doi.org/10.1002/pds.3263">https://doi.org/10.1002/pds.3263</a>.
</div>
<div id="ref-ripolloneImplicationsPropensityScore2018" class="csl-entry">
Ripollone, John E., Krista F. Huybrechts, Kenneth J. Rothman, Ryan E.
Ferguson, and Jessica M. Franklin. 2018. <span>“Implications of the
<span>Propensity Score Matching Paradox</span> in
<span>Pharmacoepidemiology</span>.”</span> <em>American Journal of
Epidemiology</em> 187 (9): 1951–61. <a href="https://doi.org/10.1093/aje/kwy078">https://doi.org/10.1093/aje/kwy078</a>.
</div>
<div id="ref-rosenbaumDesignObservationalStudies2010" class="csl-entry">
Rosenbaum, Paul R. 2010. <em>Design of Observational Studies</em>.
Springer Series in Statistics. <span>New York</span>:
<span>Springer</span>.
</div>
<div id="ref-rosenbaum2020" class="csl-entry">
———. 2020. <span>“Modern Algorithms for Matching in Observational
Studies.”</span> <em>Annual Review of Statistics and Its
Application</em> 7 (1): 143–76. <a href="https://doi.org/10.1146/annurev-statistics-031219-041058">https://doi.org/10.1146/annurev-statistics-031219-041058</a>.
</div>
<div id="ref-rosenbaum1985" class="csl-entry">
Rosenbaum, Paul R., and Donald B. Rubin. 1985a. <span>“The Bias Due to
Incomplete Matching.”</span> <em>Biometrics</em> 41 (1): 103–16. <a href="https://doi.org/10.2307/2530647">https://doi.org/10.2307/2530647</a>.
</div>
<div id="ref-rosenbaum1985a" class="csl-entry">
———. 1985b. <span>“Constructing a Control Group Using Multivariate
Matched Sampling Methods That Incorporate the Propensity Score.”</span>
<em>The American Statistician</em> 39 (1): 33. <a href="https://doi.org/10.2307/2683903">https://doi.org/10.2307/2683903</a>.
</div>
<div id="ref-rubin1973" class="csl-entry">
Rubin, Donald B. 1973. <span>“Matching to Remove Bias in Observational
Studies.”</span> <em>Biometrics</em> 29 (1): 159. <a href="https://doi.org/10.2307/2529684">https://doi.org/10.2307/2529684</a>.
</div>
<div id="ref-rubinBiasReductionUsing1980" class="csl-entry">
———. 1980. <span>“Bias <span>Reduction Using Mahalanobis-Metric
Matching</span>.”</span> <em>Biometrics</em> 36 (2): 293–98. <a href="https://doi.org/10.2307/2529981">https://doi.org/10.2307/2529981</a>.
</div>
<div id="ref-savjeInconsistencyMatchingReplacement2022" class="csl-entry">
Sävje, F. 2022. <span>“On the Inconsistency of Matching Without
Replacement.”</span> <em>Biometrika</em> 109 (2): 551–58. <a href="https://doi.org/10.1093/biomet/asab035">https://doi.org/10.1093/biomet/asab035</a>.
</div>
<div id="ref-savjeGeneralizedFullMatching2021" class="csl-entry">
Sävje, Fredrik, Michael J. Higgins, and Jasjeet S. Sekhon. 2021.
<span>“Generalized Full Matching.”</span> <em>Political Analysis</em> 29
(4): 423–47. <a href="https://doi.org/10.1017/pan.2020.32">https://doi.org/10.1017/pan.2020.32</a>.
</div>
<div id="ref-savjeQuickmatchQuickGeneralized2018" class="csl-entry">
Sävje, Fredrik, Jasjeet Sekhon, and Michael Higgins. 2018.
<em>Quickmatch: Quick Generalized Full Matching</em>. <a href="https://CRAN.R-project.org/package=quickmatch">https://CRAN.R-project.org/package=quickmatch</a>.
</div>
<div id="ref-schafer2008" class="csl-entry">
Schafer, Joseph L., and Joseph Kang. 2008. <span>“Average Causal Effects
from Nonrandomized Studies: A Practical Guide and Simulated
Example.”</span> <em>Psychological Methods</em> 13 (4): 279–313. <a href="https://doi.org/10.1037/a0014268">https://doi.org/10.1037/a0014268</a>.
</div>
<div id="ref-sekhon2011" class="csl-entry">
Sekhon, Jasjeet S. 2011. <span>“Multivariate and Propensity Score
Matching Software with Automated Balance Optimization: The Matching
Package for R.”</span> <em>Journal of Statistical Software</em> 42 (1):
1–52. <a href="https://doi.org/10.18637/jss.v042.i07">https://doi.org/10.18637/jss.v042.i07</a>.
</div>
<div id="ref-stuart2008" class="csl-entry">
Stuart, Elizabeth A. 2008. <span>“Developing Practical Recommendations
for the Use of Propensity Scores: Discussion of <span>‘</span>A Critical
Appraisal of Propensity Score Matching in the Medical Literature Between
1996 and 2003<span>’</span> by Peter Austin,Statistics in
Medicine.”</span> <em>Statistics in Medicine</em> 27 (12): 2062–65. <a href="https://doi.org/10.1002/sim.3207">https://doi.org/10.1002/sim.3207</a>.
</div>
<div id="ref-stuart2010" class="csl-entry">
———. 2010. <span>“Matching Methods for Causal Inference: A Review and a
Look Forward.”</span> <em>Statistical Science</em> 25 (1): 1–21. <a href="https://doi.org/10.1214/09-STS313">https://doi.org/10.1214/09-STS313</a>.
</div>
<div id="ref-stuart2008a" class="csl-entry">
Stuart, Elizabeth A., and Kerry M. Green. 2008. <span>“Using Full
Matching to Estimate Causal Effects in Nonexperimental Studies:
Examining the Relationship Between Adolescent Marijuana Use and Adult
Outcomes.”</span> <em>Developmental Psychology</em> 44 (2): 395–406. <a href="https://doi.org/10.1037/0012-1649.44.2.395">https://doi.org/10.1037/0012-1649.44.2.395</a>.
</div>
<div id="ref-thoemmes2011" class="csl-entry">
Thoemmes, Felix J., and Eun Sook Kim. 2011. <span>“A Systematic Review
of Propensity Score Methods in the Social Sciences.”</span>
<em>Multivariate Behavioral Research</em> 46 (1): 90–118. <a href="https://doi.org/10.1080/00273171.2011.540475">https://doi.org/10.1080/00273171.2011.540475</a>.
</div>
<div id="ref-visconti2018" class="csl-entry">
Visconti, Giancarlo, and José R. Zubizarreta. 2018. <span>“Handling
Limited Overlap in Observational Studies with Cardinality
Matching.”</span> <em>Observational Studies</em> 4 (1): 217–49. <a href="https://doi.org/10.1353/obs.2018.0012">https://doi.org/10.1353/obs.2018.0012</a>.
</div>
<div id="ref-wan2019" class="csl-entry">
Wan, Fei. 2019. <span>“Matched or Unmatched Analyses with
Propensity<span>-</span>Score<span></span>matched Data?”</span>
<em>Statistics in Medicine</em> 38 (2): 289–300. <a href="https://doi.org/10.1002/sim.7976">https://doi.org/10.1002/sim.7976</a>.
</div>
<div id="ref-wang2020" class="csl-entry">
Wang, Jixian. 2020. <span>“To Use or Not to Use Propensity Score
Matching?”</span> <em>Pharmaceutical Statistics</em>, August. <a href="https://doi.org/10.1002/pst.2051">https://doi.org/10.1002/pst.2051</a>.
</div>
<div id="ref-zakrison2018" class="csl-entry">
Zakrison, T. L., Peter C. Austin, and V. A. McCredie. 2018. <span>“A
Systematic Review of Propensity Score Methods in the Acute Care Surgery
Literature: Avoiding the Pitfalls and Proposing a Set of Reporting
Guidelines.”</span> <em>European Journal of Trauma and Emergency
Surgery</em> 44 (3): 385–95. <a href="https://doi.org/10.1007/s00068-017-0786-6">https://doi.org/10.1007/s00068-017-0786-6</a>.
</div>
<div id="ref-zubizarretaMatchingBalancePairing2014" class="csl-entry">
Zubizarreta, José R., Ricardo D. Paredes, and Paul R. Rosenbaum. 2014.
<span>“Matching for Balance, Pairing for Heterogeneity in an
Observational Study of the Effectiveness of for-Profit and
Not-for-Profit High Schools in <span>Chile</span>.”</span> <em>The
Annals of Applied Statistics</em> 8 (1): 204–31. <a href="https://doi.org/10.1214/13-AOAS713">https://doi.org/10.1214/13-AOAS713</a>.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
